{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'configargparse'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d38ed1760bc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mconfigargparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m p = configargparse.ArgParser(prog='streaming.py',\n\u001b[1;32m      4\u001b[0m                              \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Streaming Job Sample'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                              \u001b[0mdefault_config_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'configuration/run_args_streaming.conf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'configargparse'"
     ]
    }
   ],
   "source": [
    "import configargparse\n",
    "\n",
    "p = configargparse.ArgParser(prog='streaming.py',\n",
    "                             description='Streaming Job Sample',\n",
    "                             default_config_files=['configuration/run_args_streaming.conf'],\n",
    "                             formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\n",
    "p.add('--storage-account-name', type=str, required=True,\n",
    "      help='Azure Storage account name (used for data output and checkpointing)')\n",
    "p.add('--storage-account-key', type=str, required=True,\n",
    "      help='Azure Storage key', env_var='STREAMING_STORAGE_KEY')\n",
    "p.add('--storage-container-name', type=str, required=False, default='data',\n",
    "      help='Azure Storage container name')\n",
    "p.add('--output-path', type=str, required=False, default=\"delta/streaming-data/\",\n",
    "      help='Path to stream output storage location (deltalake) relative to container''s root')\n",
    "p.add('--input-eh-connection-string', type=str, required=True,\n",
    "      help='Input Event Hub connection string', env_var='STREAMING_INPUT_EH_CONNECTION_STRING')\n",
    "p.add('--max-events-per-trigger', type=int, required=False, default=10000,\n",
    "      help='Metering points to read per trrigger interval')\n",
    "p.add('--trigger-interval', type=str, required=False, default='1 second',\n",
    "      help='Trigger interval to generate streaming batches (format: N seconds)')\n",
    "p.add('--streaming-checkpoint-path', type=str, required=False, default=\"checkpoints/streaming\",\n",
    "      help='Path to checkpoint folder for streaming')\n",
    "p.add('--telemetry-instrumentation-key', type=str, required=True,\n",
    "      help='Instrumentation key used for telemetry')\n",
    "\n",
    "\n",
    "args, unknown_args = p.parse_known_args()\n",
    "\n",
    "if unknown_args:\n",
    "    print(\"Unknown args:\")\n",
    "    _ = [print(arg) for arg in unknown_args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_conf = SparkConf(loadDefaults=True) \\\n",
    "    .set('fs.azure.account.key.{0}.dfs.core.windows.net'.format(args.storage_account_name),\n",
    "         args.storage_account_key)\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=spark_conf)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(\"Spark Configuration:\")\n",
    "_ = [print(k + '=' + v) for k, v in sc.getConf().getAll()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_eh_starting_position = {\n",
    "    \"offset\": \"-1\",         # starting from beginning of stream\n",
    "    \"seqNo\": -1,            # not in use\n",
    "    \"enqueuedTime\": None,   # not in use\n",
    "    \"isInclusive\": True\n",
    "}\n",
    "input_eh_connection_string = args.input_eh_connection_string\n",
    "input_eh_conf = {\n",
    "    # Version 2.3.15 and up requires encryption\n",
    "    'eventhubs.connectionString': \\\n",
    "    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(input_eh_connection_string),\n",
    "    'eventhubs.startingPosition': json.dumps(input_eh_starting_position),\n",
    "    'maxEventsPerTrigger': args.max_events_per_trigger,\n",
    "}\n",
    "\n",
    "print(\"Input event hub config:\", input_eh_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}