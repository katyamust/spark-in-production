{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import configargparse\n",
    "from pathlib import Path\n",
    "from spark_utils.streaming_utils import event_hub_parse, preview_stream\n",
    "\n",
    "p = configargparse.ArgParser(prog='streaming.py',\n",
    "                             description='Streaming Job Sample',\n",
    "                             default_config_files=[Path().joinpath('configuration/run_args_data_generator.conf').resolve().as_posix()],\n",
    "                             formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\n",
    "p.add('--output-eh-connection-string', type=str, required=True,\n",
    "      help='Output Event Hub connection string', env_var='GENERATOR_OUTPUT_EH_CONNECTION_STRING')\n",
    "\n",
    "args, unknown_args = p.parse_known_args()\n",
    "\n",
    "if unknown_args:\n",
    "    print(\"Unknown args:\")\n",
    "    _ = [print(arg) for arg in unknown_args]\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_conf = SparkConf(loadDefaults=True)\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(conf=spark_conf)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(\"Spark Configuration:\")\n",
    "_ = [print(k + '=' + v) for k, v in sc.getConf().getAll()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rateStream = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"rate\") \\\n",
    "  .option(\"rowsPerSecond\", 10) \\\n",
    "  .load()\n",
    "\n",
    "preview_stream(rateStream, await_seconds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, struct\n",
    "\n",
    "generatedData = rateStream \\\n",
    "   .withColumn(\"value\", col(\"value\") * 3019) \\\n",
    "   .withColumnRenamed(\"timestamp\", \"ObservationTime\") \\\n",
    "   .withColumn(\"MeterId\", col(\"value\") % lit(127)) \\\n",
    "   .withColumn(\"SupplierId\", col(\"value\") % lit(23)) \\\n",
    "   .withColumn(\"Measurement\", struct(\n",
    "       (col(\"value\") % lit(59)).alias(\"Value\"),\n",
    "       lit(\"kWH\").alias(\"Unit\")\n",
    "   )) \\\n",
    "   .drop(\"value\")\n",
    "\n",
    "preview_stream(generatedData,await_seconds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "jsonData = generatedData \\\n",
    "    .select(to_json(struct(col(\"*\"))).cast(\"string\").alias(\"body\"))\n",
    "\n",
    "preview_stream(jsonData, await_seconds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_utils.schemas import message_schema\n",
    "from pyspark.sql.functions import col, from_json\n",
    "fromjsondata = jsonData \\\n",
    "            .select(from_json(col(\"body\").cast(\"string\"), message_schema).alias(\"message\")) \\\n",
    "            .select(col(\"message.*\"))\n",
    "\n",
    "preview_stream(fromjsondata, await_seconds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "eh_conf = {\n",
    "    'eventhubs.connectionString':\n",
    "    sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(args.output_eh_connection_string)\n",
    "}\n",
    "\n",
    "exec = jsonData \\\n",
    "    .writeStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**eh_conf) \\\n",
    "    .option(\"checkpointLocation\", '.checkpoint/data-generator3') \\\n",
    "    .start()\n",
    "\n",
    "exec.awaitTermination()\n",
    "exec.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}