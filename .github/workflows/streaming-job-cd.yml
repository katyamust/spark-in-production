name: Streaming Job Deploy (Enrichment and Validation)
on:
  # push:
  #   branches:
  #     - main
  #   paths:
  #     - build/terraform/databricks_streaming_job/**
  #     - build/terraform/job_modules/**
  #     - .github/workflows/streaming-job-cd.yml 
  workflow_dispatch:
    inputs:
      resource_group_name:
        description: 'Resource Group Name You Want to Use for Deployment'
        required: true
        default: ''
      env_name:
        description: 'Env name used to postfix resources names'
        required: true
        default: ''
      organisation_name:
        description: 'Organisation name to postfix resources names'
        required: true
        default: ''
      client_id:
        description: 'Service Principal Client (Application) Id'
        required: true
        default: ''
      client_secret:
        description: 'Service Principal Secret'
        required: true
        default: ''

jobs:
  streaming_job_deploy:
    
    # Name the Job
    name: Deploy databricks cluster and create job
    # Set the type of machine to run on
    runs-on: ubuntu-latest
    env:
      WHEEL_FILE: "dbfs:/wheels/spark_utils-1.0-py3-none-any.whl"
      MAIN_PYTHON_FILE: "dbfs:/streaming/streaming_job.py"

    steps:
      
      - name: Checkout code
        uses: actions/checkout@v2
      
      - name: Read Pipeline Configuration
        uses: ./.github/actions/read-pipeline-configuration

      - name: Set Environment Secrets
        run: |  
          echo "ARM_TENANT_ID=${{ secrets.TENANT_ID }}" >> $GITHUB_ENV
          echo "ARM_CLIENT_ID=${{ secrets[env.GITHUB_SECRET_NAME_SPN_ID] }}" >> $GITHUB_ENV
          echo "ARM_CLIENT_SECRET=${{ secrets[env.GITHUB_SECRET_NAME_SPN_SECRET] }}" >> $GITHUB_ENV
          echo "ARM_SUBSCRIPTION_ID=${{ secrets[env.GITHUB_SECRET_NAME_SUBSCRIPTION_ID] }}" >> $GITHUB_ENV

      # Run only if workflow started on demand (manually triggered)
      - name: Set Variables from Pipeline Inputs
        uses: ./.github/actions/override_env_settings
        if:   github.event.inputs.resource_group_name
    
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v1.2.1
        with:
          terraform_wrapper: false
      
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.7' # Version range or exact version of a Python version to use, using SemVer's version range syntax
          architecture: 'x64' # optional x64 or x86. Defaults to x64 if not specified

      - name: Azure CLI Install and Login
        uses: ./.github/actions/azure-cli-install-login

      - name: Obtain Databricks Workspace ID and Url
        id: obtain-db-id-url
        uses: ./.github/actions/obtain-databricks-id-url
        
      - name: Databricks CLI Install And Connect
        uses: ./.github/actions/databricks-cli-install-connect
        with:
          workspace-url: ${{ steps.obtain-db-id-url.outputs.workspace-url }}

      - name: Obtain Keyvault ID and Name
        id: obtain-keyvault-id-name 
        uses: ./.github/actions/obtain-keyvault-id-name

      - name: Create Python Wheel for Databricks Jobs
        working-directory: ./src/python/src
        run: |
          pip install wheel
          python setup.py sdist bdist_wheel
        
      - name: Copy Wheel File to DBFS
        run: |    
          dbfs cp --overwrite -r ./src/python/src/dist/spark_utils-1.0-py3-none-any.whl ${{ env.WHEEL_FILE }}
          
      - name: Copy Job Definition to Databricks File System (DBFS)
        id: copy_main_file_todbfs
        run: |    
          dbfs cp --overwrite ./src/python/src/streaming_job.py ${{ env.MAIN_PYTHON_FILE }}

      # Try not to reference TF_VAR variables in pipeline yml files, only values should be set and they should be read in terraform only
      # rather create duplicate ENV pipeline vatiable if needed to separate concerns
      - name: Set TF Vars
        run: |
          echo "TF_VAR_environment=${{ env.ENV_NAME }}" >> $GITHUB_ENV 
          echo "TF_VAR_resource_group_name=${{ env.RESOURCE_GROUP_NAME }}" >> $GITHUB_ENV
          echo "TF_VAR_storage_account_name=timeseriesdata${{ env.ENV_NAME }}" >> $GITHUB_ENV
          echo "TF_VAR_keyvault_id=${{ steps.obtain-keyvault-id-name.outputs.keyvault-id }}" >> $GITHUB_ENV
          echo "TF_VAR_databricks_id=${{ steps.obtain-db-id-url.outputs.workspace-id }}" >> $GITHUB_ENV
          echo "TF_VAR_python_main_file=${{ env.MAIN_PYTHON_FILE}}" >> $GITHUB_ENV
          echo "TF_VAR_wheel_file=${{ env.WHEEL_FILE}}" >> $GITHUB_ENV  

      - name: Configure Terraform Backend
        uses: ./.github/actions/configure-terraform-backend
        with:
          backend-file-path: "./build/terraform/databricks_streaming_job/backend.tf"
          resource-group-name: "${{ env.RESOURCE_GROUP_NAME }}"
          storage-account-name: "tfstate${{ env.ENV_NAME }}"

      # Create streaming job
      - name: Terraform Databricks Init
        working-directory: ./build/terraform/databricks_streaming_job
        run: terraform init

      # resource in command must match with resource name in main.tf
      # after feature: https://github.com/databrickslabs/terraform-provider-databricks/issues/389 
      # is available this should be changed and job should not be recreated on each run - taint to be removed           
      - name: Terraform Databricks Apply
        id: terraform-apply
        working-directory: ./build/terraform/databricks_streaming_job
        run: |
          terraform apply -no-color -auto-approve
          terraform taint module.streaming_job.databricks_job.streaming_job
          echo "::set-output name=job-id::$(terraform output databricks_job_id)"
        continue-on-error: false

      - name: Databricks CLI Run the Job
        id: run-job
        uses: ./.github/actions/databricks-cli-run-jobs
        with:
          job-ids: ${{ steps.terraform-apply.outputs.job-id }}
          
      - name: Check Job Status
        working-directory: ./build
        run: |
          pip install configargparse
          pip install requests
          python -u job_status_check.py --job-run-ids  ${{ steps.run-job.outputs.job-run-ids }} --retries 2 --databricks-url 'https://${{ steps.obtain-db-id-url.outputs.workspace-url }}' --token ${{ env.DATABRICKS_AAD_TOKEN }}
